{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of this notebook is simple exploratory analysis to identify conditions for word removal.\n",
    "Given OCR is not completely reliable, this notebook will work to remove common occurrences that may altar the output of a topic model. It does this by ingesting all the parsed OCR, removing special characters, removing stop words, stemming, and then doing simple counts to identify popular edge cases to remove. This is so I can apply the same cleaning logic to all the documents instead of analyzing the output of each document individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json \n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pathlib import Path\n",
    "import regex as re\n",
    "\n",
    "def create_fh_logger(file):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "    file_handler = logging.FileHandler(file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations of json files + a place to store a log\n",
    "src = Path.cwd().parent.parent / 'processing' / 'nro_declassified' / 'ocr'\n",
    "files = list(src.glob('*json'))\n",
    "logs = Path.cwd().parent.parent / 'processing' / 'logs'\n",
    "logs.mkdir(exist_ok=True)\n",
    "logger = create_fh_logger(logs / \"analyze_ocr.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    \"\"\"This is put into a function to add additional cleaning mechanisms if needed\n",
    "\n",
    "    :param txt: _description_\n",
    "    :type txt: _type_\n",
    "    :return: _description_\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    txt = re.sub(\"[^A-Za-z0-9 ]+\", '', txt)\n",
    "    txt = txt.lower()\n",
    "    return txt\n",
    "stopword = stopwords.words('english') # retrieve the stopwords\n",
    "stopword.extend(['secret', 'fop', 'top', 'classified', 'declassified', 'approved', 'release', 'dod', 'general', 'page', 'via', 'would', 'throughout', 'director', 'chief'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    logger.info(f'Analyzing {str(file).encode(\"utf-8\")}')\n",
    "    data = data[file.stem] # foorgot why but it's a nested json with pdf name as first key\n",
    "    for pg_num in data.keys():\n",
    "        dat = data[pg_num]['text']\n",
    "        dat = [word for word in dat if word != '']\n",
    "        dat = [clean_text(word) for word in dat]\n",
    "        dat = [word for word in dat if word not in stopword and word != '' and word != ' ']\n",
    "        words.extend(dat)\n",
    "    logger.info(f'Finished analyzing {str(file).encode(\"utf-8\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmed = [stemmer.stem(word) for word in words] # stimmy\n",
    "stemmed = [word for word in stemmed if not all(char == word[0] for char in word)] # remove words where the word is the same character\n",
    "stemmed = [word for word in stemmed if len(word) > 2 and word.isalpha()] # has to be a word greater than 2 characters with all letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = Counter(stemmed) # create a dict of word: count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(word_freqs) # creates a wordcloud from the previously created counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(word_freqs.keys())} distinct words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
