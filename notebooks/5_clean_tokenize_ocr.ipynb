{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs the final word tokenization for the LDA Topic Model. It uses cases derived from analyzing the output. Unlike the previous, this is now operating one file at a time. Also, this saves the original bounding box and confidence score of the OCR'd text. This is to help track the translations through the model to a physical location on the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pathlib import Path\n",
    "import regex as re\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils import create_fh_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations of json files + a place to store a log\n",
    "src = Path.cwd().parent.parent / 'processing' / 'nro_declassified' / 'ocr'\n",
    "dst = src.parent / 'tokenized'\n",
    "files = list(src.glob('*json'))\n",
    "logs = src.parent.parent / 'logs'\n",
    "logs.mkdir(exist_ok=True)\n",
    "logger = create_fh_logger(logs / \"tokenize_ocr.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    \"\"\"This is put into a function to add additional cleaning mechanisms if needed\n",
    "\n",
    "    :param txt: _description_\n",
    "    :type txt: _type_\n",
    "    :return: _description_\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    txt = re.sub(\"[^A-Za-z0-9 ]+\", '', txt)\n",
    "    txt = txt.lower()\n",
    "    return txt\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stopword = stopwords.words('english') # retrieve the stopwords\n",
    "stopword.extend(['secret', 'fop', 'top', 'classified', 'declassified', 'approved', 'release', 'dod', 'general', 'page', 'via', 'would', 'throughout', 'director', 'chief', 'page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    logger.info(f'Analyzing {str(file).encode(\"utf-8\")}')\n",
    "    data = data[file.stem] # foorgot why but it's a nested json with pdf name as first key\n",
    "    doc = []\n",
    "    for pg_num in data.keys():\n",
    "        words = data[pg_num]['text']\n",
    "        for i in range(0, len(words)):\n",
    "            (x, y, w, h) = (data[pg_num]['left'][i], data[pg_num]['top'][i], data[pg_num]['width'][i], data[pg_num]['height'][i])\n",
    "            conf = data[pg_num]['conf'][i]\n",
    "            word = clean_text(words[i])\n",
    "            word = word if word not in stopword else ''\n",
    "            word = stemmer.stem(word)\n",
    "            word = word if not all(char == word[0] for char in word) else '' # remove words where the word is the same character\n",
    "            if word != '' and word != ' ' and len(word) > 2 and word.isalpha():\n",
    "                item = {'pg': pg_num, 'word': word, 'x': x, 'y': y, 'w': w, 'h': h, 'conf': conf}\n",
    "                doc.append(item)\n",
    "    with open(dst / f'{file.stem}.json', 'w') as f:\n",
    "        json.dump(doc, f)\n",
    "    logger.info(f'Finished analyzing {str(file).encode(\"utf-8\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
