{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rotate the images and remove impossible to OCR images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from utils import create_fh_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations of json files \n",
    "src = Path.cwd().parent.parent.parent.parent / 'processing' / 'nro_declassified' / 'ocr'\n",
    "files = list(src.glob('*json'))\n",
    "output_loc = src.parent / 'good_docs'\n",
    "output_loc.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data = data[file.stem] # foorgot why but it's a nested json with pdf name as first key\n",
    "    for pg_num in data.keys():\n",
    "        dat = data[pg_num]['conf']\n",
    "        confs.extend(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.174540e+07\n",
       "mean     4.790513e+01\n",
       "std      4.216411e+01\n",
       "min     -1.000000e+00\n",
       "25%     -1.000000e+00\n",
       "50%      4.800000e+01\n",
       "75%      9.500000e+01\n",
       "max      9.700000e+01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(confs).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, most of our data isn't acceptable. Is it file based (i.e., we should just remove files) or is it spread throughout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = {}\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data = data[file.stem] # foorgot why but it's a nested json with pdf name as first key\n",
    "    confs[file.stem] = []\n",
    "    for pg_num in data.keys():\n",
    "        dat = data[pg_num]['conf']\n",
    "        confs[file.stem].extend(dat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create descriptive statistics for each file and see where the 50% quartile is at with ocr confidence\n",
    "bad_docs = []\n",
    "for key, val in confs.items():\n",
    "    doc_confs = np.array(val)\n",
    "    q50 = np.median(doc_confs)\n",
    "    if q50 < 90:\n",
    "        bad_docs.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1354 bad documents\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(bad_docs)} bad documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create descriptive statistics for each file and see where the 50% quartile is at with ocr confidence\n",
    "bad_docs = []\n",
    "for key, val in confs.items():\n",
    "    doc_confs = np.array(val)\n",
    "    q50 = np.median(doc_confs)\n",
    "    if q50 < 70:\n",
    "        bad_docs.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 952 awful documents\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(bad_docs)} awful documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 681 unsalvageable documents\n"
     ]
    }
   ],
   "source": [
    "# let's create descriptive statistics for each file and see where the 50% quartile is at with ocr confidence\n",
    "bad_docs = []\n",
    "for key, val in confs.items():\n",
    "    doc_confs = np.array(val)\n",
    "    q50 = np.median(doc_confs)\n",
    "    if q50 < 50:\n",
    "        bad_docs.append(key)\n",
    "print(f'There are {len(bad_docs)} unsalvageable documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create descriptive statistics for each file and see where the 50% quartile is at with ocr confidence\n",
    "bad_docs = []\n",
    "good_docs = []\n",
    "for key, val in confs.items():\n",
    "    doc_confs = np.array(val)\n",
    "    q50 = np.median(doc_confs)\n",
    "    if q50 < 90:\n",
    "        bad_docs.append(key)\n",
    "    else:\n",
    "        good_docs.append(key) # let's only keep the good documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_loc / 'analyze.json', 'w') as f:\n",
    "    f.write(json.dumps({'documents': good_docs}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
