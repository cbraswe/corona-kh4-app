{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/doma/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c\n",
    "# https://medium.com/@yashj302/spell-check-and-correction-nlp-python-f6a000e3709d better spell echeck\n",
    "# https://gist.github.com/yuchenlin/a2f42d3c4378ed7b83de65c7a2222eb2\n",
    "from difflib import SequenceMatcher\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from spylls.hunspell import Dictionary #https://github.com/zverok/spylls\n",
    "import regex as re \n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets read in the files we will continue to work on\n",
    "src = Path.cwd().parent.parent.parent.parent / 'processing' / 'nro_declassified' / 'bert_ocr'\n",
    "files = list(src.glob('*json'))\n",
    "output_loc = src.parent / 'bert_results'\n",
    "output_loc.mkdir(exist_ok=True)\n",
    "good = [file for file in files]\n",
    "dictionary = Dictionary.from_files('en_US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_sent(text, top_k=5):\n",
    "    text = \"[CLS] %s [SEP]\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    masked_index = [idx for idx, token in enumerate(tokenized_text) if token =='[MASK]']\n",
    "    preds = []\n",
    "    for idx, mask in enumerate(masked_index): \n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens_tensor)\n",
    "            predictions = outputs[0]\n",
    "        probs = torch.nn.functional.softmax(predictions[0, mask], dim=-1)\n",
    "        top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "        prediction = []\n",
    "        for i, pred_idx in enumerate(top_k_indices):\n",
    "            predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "            token_weight = top_k_weights[i]\n",
    "            print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "            prediction.append(predicted_token)\n",
    "        preds.append(prediction)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]: 'generated'  | weights: 0.08679242432117462\n",
      "[MASK]: 'provided'  | weights: 0.06315764784812927\n",
      "[MASK]: 'includes'  | weights: 0.3903972804546356\n",
      "[MASK]: 'contains'  | weights: 0.22848112881183624\n",
      "[MASK]: 'based'  | weights: 0.6670317649841309\n",
      "[MASK]: 'relying'  | weights: 0.2214537411928177\n"
     ]
    }
   ],
   "source": [
    "original_text = (\"The BERT pre-trained language model is useful for predicting multiple viable replacements for the masked words. \" \n",
    "    \"With that said, the model is not aware of any characters uncovered by OCR. We can augment this deficiency with our suggested word\" \n",
    "    \" list from SpellChecker, which incorporates characters from the garbled OCR output. Combining BERT’s context-based suggtion with\" \n",
    "    \" SpellChecker’s word-based suggestions  better predictions than relying solely on BERT\")\n",
    "\n",
    "masked_text = (\"The BERT pre-trained language model is useful for predicting multiple viable replacements for the masked words. \" \n",
    "    \"With that said, the model is not aware of any characters [MASK] by OCR. We can augment this deficiency with our suggested word\" \n",
    "    \" list from SpellChecker, which [MASK] characters from the garbled OCR output. Combining BERT’s context-based suggtion with\" \n",
    "    \" SpellChecker’s word-based suggestions yield better predictions than [MASK] solely on BERT\")\n",
    "predictions = predict_masked_sent(masked_text, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]: 'generated'  | weights: 0.08679242432117462\n",
      "[MASK]: 'includes'  | weights: 0.3903972804546356\n",
      "[MASK]: 'based'  | weights: 0.6670317649841309\n"
     ]
    }
   ],
   "source": [
    "peds = predict_masked_sent(masked_text, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    results = {}\n",
    "    # let's open it, open the text, and join it all so we can use BERT to create a token\n",
    "    # then we need to grab the mask ids\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    doc_words = []\n",
    "    for pg_num in data.keys():\n",
    "        doc_words.extend( data[pg_num]['text'] )\n",
    "    print(file)\n",
    "    # we could do it by paragraph or max token length, we'll do it by both\n",
    "    doc_text = (' ').join(doc_words)\n",
    "    doc_text = re.sub(\"\\s\\s+\" , \" \", doc_text)\n",
    "    max_token = 1500 # bad\n",
    "    results = []\n",
    "    for idx in range(0, len(doc_text), max_token):\n",
    "        text = ('').join(doc_text[idx: idx+max_token])\n",
    "        if '[MASK]' in text:\n",
    "            try:\n",
    "                predictions = predict_masked_sent(text, top_k=1)\n",
    "                tokenized_text = text.split(' ')\n",
    "                masked_index = [idx for idx, token in enumerate(tokenized_text) if token =='[MASK]']\n",
    "                for idx, mask in enumerate(masked_index): \n",
    "                    tokenized_text[mask] = predictions[idx][0]\n",
    "                replaced = (' ').join([str(t) for t in tokenized_text])\n",
    "                results.append({'text': text, 'predictions': replaced})\n",
    "            except Exception as e:\n",
    "                print(f'Exception encountered: {e} with {str(file.name)}')\n",
    "\n",
    "    with open(output_loc / file.name, 'w') as f:\n",
    "        f.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'c:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\bert_results\\1958-04-16MEMO DEAR GOODPASTER COVER LETTER PROJECT CORONA_812.json', 'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data(data: dict):\n",
    "    max_tokens = 5 # it has to use tokens instead of words since [MASK] is not a one to one index\n",
    "    text = data['text'].split(' ')\n",
    "    pred = data['predictions'].split(' ')\n",
    "    masked_idx = [i for i, t in enumerate(text) if t == '[MASK]']\n",
    "    for idx, mask in enumerate(masked_idx):\n",
    "        print('\\n\\nSubmitted Text:\\n')\n",
    "        start = mask - max_tokens\n",
    "        end = mask + max_tokens\n",
    "        t = (' ').join(text[start:end])\n",
    "        p = (' ').join(pred[start:end])\n",
    "        print(f\"{t}\")\n",
    "        print('\\n\\nGenerated Text:\\n')\n",
    "        print(f\"{p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Submitted Text:\n",
      "\n",
      "sites under construction previously assbeerved [MASK] OR other Major installations\n",
      "\n",
      "\n",
      "Generated Text:\n",
      "\n",
      "sites under construction previously assbeerved sites OR other Major installations\n",
      "\n",
      "\n",
      "Submitted Text:\n",
      "\n",
      "As the Seviet far North [MASK] Sots 3 program IT\n",
      "\n",
      "\n",
      "Generated Text:\n",
      "\n",
      "As the Seviet far North and Sots 3 program IT\n"
     ]
    }
   ],
   "source": [
    "display_data(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'c:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\bert_results\\1958-07-23DOC PROGRESS REPORT ON MILITARY RECONNAISSANCE SATELLITE PRO_2312.json'\n",
    "with open(file, 'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Submitted Text:\n",
      "\n",
      "for release 20231018 CO51449 29 [MASK] [MASK] July 23 1958\n",
      "\n",
      "\n",
      "Generated Text:\n",
      "\n",
      "for release 20231018 CO51449 29 approved issued July 23 1958\n",
      "\n",
      "\n",
      "Submitted Text:\n",
      "\n",
      "release 20231018 CO51449 29 [MASK] [MASK] July 23 1958 memorandum\n",
      "\n",
      "\n",
      "Generated Text:\n",
      "\n",
      "release 20231018 CO51449 29 approved issued July 23 1958 memorandum\n"
     ]
    }
   ],
   "source": [
    "display_data(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r'c:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\bert_results\\1958-09-30MEMO STAFF MEETING MINUTES 30 SEPTEMBER 1958 (CORONA ITEMS) _409.json'\n",
    "with open(file, 'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Submitted Text:\n",
      "\n",
      "need to Be reexamined He [MASK] QM to undertake this\n",
      "\n",
      "\n",
      "Generated Text:\n",
      "\n",
      "need to Be reexamined He asked QM to undertake this\n",
      "\n",
      "\n",
      "Submitted Text:\n",
      "\n",
      "[MASK] QM to undertake this [MASK] and suggested that later\n",
      "\n",
      "\n",
      "Generated Text:\n",
      "\n",
      "asked QM to undertake this study and suggested that later\n"
     ]
    }
   ],
   "source": [
    "display_data(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
