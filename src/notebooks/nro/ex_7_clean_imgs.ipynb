{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will use craft to detect if text should be rotated prior to using tesseract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import numpy as np \n",
    "from pathlib import Path \n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from utils import create_fh_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = Path.cwd().parent.parent.parent.parent / 'processing' / 'nro_declassified' / 'imgs'\n",
    "dst = src.parent / 'cleaned_imgs'\n",
    "dst.mkdir(parents=True, exist_ok=True)\n",
    "east_weights = dst.parent.parent / 'models' / 'frozen_east_text_detection.pb'\n",
    "logs = src.parent.parent / 'logs'\n",
    "logs.mkdir(exist_ok=True)\n",
    "logger = create_fh_logger(logs / \"cleaned_imgs.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Identify Rotations: 0, 90, 180, 270\n",
    "The code below uses EDGE text detection to identify text for an image un-rotated, and then rotated: 90, 180, or 270 degrees. Then, non-maximum supression to filter out duplicative detections. Lastly, it does a count on detections for that rotation. \n",
    "\n",
    "The detection count per rotation will be used to identify the ideal rotation for the entire pdf. Since mirrored rotations perform similarly, a rotation will only be performed if the rotated detections are at least 10% more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/opencv/opencv/blob/7fb70e170154d064ef12d8fec61c0ae70812ce3d/samples/dnn/text_detection.py\n",
    "def decode(scores, geometry, scoreThresh):\n",
    "    detections = []\n",
    "    confidences = []\n",
    "    height = scores.shape[2]\n",
    "    width = scores.shape[3]\n",
    "    for y in range(0, height):\n",
    "        scoresData = scores[0][0][y]\n",
    "        x0_data = geometry[0][0][y]\n",
    "        x1_data = geometry[0][1][y]\n",
    "        x2_data = geometry[0][2][y]\n",
    "        x3_data = geometry[0][3][y]\n",
    "        anglesData = geometry[0][4][y]\n",
    "        for x in range(0, width):\n",
    "            score = scoresData[x]\n",
    "            if(score < scoreThresh):\n",
    "                continue\n",
    "            offsetX = x * 4.0\n",
    "            offsetY = y * 4.0\n",
    "            angle = anglesData[x]\n",
    "            cosA = math.cos(angle)\n",
    "            sinA = math.sin(angle)\n",
    "            h = x0_data[x] + x2_data[x]\n",
    "            w = x1_data[x] + x3_data[x]\n",
    "            offset = ([offsetX + cosA * x1_data[x] + sinA * x2_data[x], offsetY - sinA * x1_data[x] + cosA * x2_data[x]])\n",
    "            p1 = (-sinA * h + offset[0], -cosA * h + offset[1])\n",
    "            p3 = (-cosA * w + offset[0],  sinA * w + offset[1])\n",
    "            center = (0.5*(p1[0]+p3[0]), 0.5*(p1[1]+p3[1]))\n",
    "            detections.append((center, (w,h), -1*angle * 180.0 / math.pi))\n",
    "            confidences.append(float(score))\n",
    "    return [detections, confidences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_hw = 640\n",
    "conf_threshold = .2\n",
    "nms_threshold=.2\n",
    "available_rotations = [cv2.ROTATE_180, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, -1]\n",
    "rotations = {key: 0 for key in available_rotations} # lets use this as an accumulator to count the amount of text matches per rotation to be used as our final determinator for how to rotate the image\n",
    "net = cv2.dnn.readNetFromTensorflow(str(east_weights))\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "outNames = [\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"]\n",
    "for image in list(Path(rotation_test).glob('*png')):\n",
    "    name = image.name\n",
    "    image = cv2.imread(str(image))\n",
    "    for rotation in available_rotations:\n",
    "        frame = image.copy()\n",
    "        frame = cv2.rotate(frame, rotation) if rotation >=0 else frame\n",
    "        height_ = frame.shape[0]\n",
    "        width_ = frame.shape[1]\n",
    "        rW = width_ / float(resize_hw)\n",
    "        rH = height_ / float(resize_hw)\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (resize_hw, resize_hw), (123.68, 116.78, 103.94), True, False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(outNames)\n",
    "\n",
    "        # Get scores and geometry\n",
    "        scores = outs[0]\n",
    "        geometry = outs[1]\n",
    "        [boxes, confidences] = decode(scores, geometry, conf_threshold)\n",
    "        indices = cv2.dnn.NMSBoxesRotated(boxes, confidences, conf_threshold, nms_threshold)\n",
    "        rotations[rotation] += len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
