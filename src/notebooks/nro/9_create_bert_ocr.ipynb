{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/doma/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c\n",
    "# https://medium.com/@yashj302/spell-check-and-correction-nlp-python-f6a000e3709d better spell echeck\n",
    "import json\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
    "import regex as re \n",
    "from spylls.hunspell import Dictionary #https://github.com/zverok/spylls\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from utils import create_fh_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations of json files \n",
    "good_docs =  Path.cwd().parent.parent.parent.parent / 'processing' / 'nro_declassified' / 'good_docs'\n",
    "src = Path.cwd().parent.parent.parent.parent / 'processing' / 'nro_declassified' / 'ocr'\n",
    "files = list(src.glob('*json'))\n",
    "output_loc = src.parent / 'good_docs'\n",
    "output_loc.mkdir(exist_ok=True)\n",
    "dest = Path.cwd().parent.parent.parent.parent / 'processing' / 'nro_declassified' / 'bert_ocr'\n",
    "dest.mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary.from_files('en_US')\n",
    "def validate_word(word, confidence, next_words):\n",
    "    og = word\n",
    "    word = word.lower()\n",
    "    valid = dictionary.lookup(word)\n",
    "    try:\n",
    "        suggestion = list(dictionary.suggest(word))[0]\n",
    "    except:\n",
    "        suggestion = ''\n",
    "    distance = nltk.edit_distance(word, suggestion)\n",
    "    if word == suggestion.lower() or ( distance < 1 and not valid):\n",
    "        return suggestion\n",
    "    elif not valid and confidence < .8:\n",
    "        # mask it for bert\n",
    "        return '[MASK]'\n",
    "    else:\n",
    "        # keep it\n",
    "        return og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets read in the files we will continue to work on\n",
    "with open( good_docs / 'analyze.json','r') as f:\n",
    "    docs = json.loads(f.read())['documents']\n",
    "good = [file for file in files if file.stem in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\ocr\\1956-06-26MEMO PHYSICAL RECOVERY OF SATELLITE PAYLOADS A PRELIMINARY I_586.json\n",
      "36\n",
      "0\n",
      "1\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "2\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "3\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "c:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\ocr\\1957-05-16MEMO DATA SHEET ON EARTH SATELLITE PROJECT (AIR FORCE 117L)_2307.json\n",
      "2\n",
      "0\n",
      "1\n",
      "c:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\ocr\\1957-09-27EXCERPTS FROM MEMORANDUM FOR RECORD BY LB KIRKPATRICK ON SUB_587.json\n",
      "1\n",
      "0\n",
      "c:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\ocr\\1957-11-12DOC RAND RESEARCH MEMORANDUM A FAMILY OF RECOVERABLE RECONNA_588.json\n",
      "128\n",
      "0\n",
      "1\n",
      "10\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "11\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "12\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "2\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "3\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "examples = {}\n",
    "for file in good:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data = data[file.stem] \n",
    "    print(file)\n",
    "    print(len(data.keys()))\n",
    "    for pg_num in data.keys():\n",
    "        words = data[pg_num]['text'] \n",
    "        conf = data[pg_num]['conf']\n",
    "        print(pg_num)\n",
    "        for idx, word in enumerate(words):\n",
    "            if word != '':\n",
    "                word = re.sub(\"[^A-Za-z0-9 ]+\", '', word)\n",
    "                replacement = validate_word(word, conf[idx], next_words=words[idx:idx+2])\n",
    "                if replacement != word:\n",
    "                    examples[word] = replacement\n",
    "                data[pg_num]['text'][idx] = replacement\n",
    "    # save the masked data\n",
    "    with open(dest / file.name, 'w') as f:\n",
    "        f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#Predict words for mask using BERT; \n",
    "from difflib import SequenceMatcher\n",
    "#refine prediction by matching with proposals from SpellChecker\n",
    "def predict_word(text_original, predictions, maskids): # MEDIUM: \n",
    "    for i in range(len(maskids)):\n",
    "        preds = torch.topk(predictions[0, maskids[i]], k=50) \n",
    "        indices = preds.indices.tolist()\n",
    "        simmax = 0\n",
    "        predicted_token = ''\n",
    "        for pred_word in tokenizer.convert_ids_to_tokens(indices):\n",
    "            for spell_checked in list(dictionary.suggest(word)):\n",
    "                s = SequenceMatcher(None, pred_word, spell_checked).ratio()\n",
    "                if s is not None and s > simmax:\n",
    "                    simmax = s\n",
    "                    predicted_token = pred_word\n",
    "        text_original = text_original.replace('[MASK]', predicted_token, 1)\n",
    "    return text_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(dest.glob('*json')) # these are ready for bert\n",
    "bert_results = dest.parent / 'bert_results'\n",
    "bert_results.mkdir(exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for file in files[:3]:\n",
    "    # let's open it, open the text, and join it all so we can use BERT to create a token\n",
    "    # then we need to grab the mask ids\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    doc_words = []\n",
    "    for pg_num in data.keys():\n",
    "        doc_words.extend( data[pg_num]['text'] )\n",
    "    # we could do it by paragraph or max token length, we'll do it by both\n",
    "    doc_text = (' ').join(words)\n",
    "    doc_text = re.sub(\"\\s\\s+\" , \" \", doc_text)\n",
    "    max_token = 500 # bad\n",
    "    for idx in range(0, len(doc_text), max_token):\n",
    "        text = ('').join(doc_text[idx: idx+max_token])\n",
    "        # let's add periods every 10 words\n",
    "        words = []\n",
    "        if '.' not in text:\n",
    "            for idx, word in enumerate(text.split(' ')):\n",
    "                if idx%10 == 0 and idx != 0: # bad\n",
    "                    words.extend(['.', word])\n",
    "                else:\n",
    "                    words.append(word)\n",
    "            words.append('.')\n",
    "            text = (' ').join(words)\n",
    "        tokened = tokenizer.tokenize(text)\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokened)\n",
    "        masked_idx = [i for i, e in enumerate(tokened) if e == '[MASK]']\n",
    "        if len(masked_idx) > 0:\n",
    "            segs = [i for i, e in enumerate(tokened) if e == \".\"]\n",
    "            segments_ids=[]\n",
    "            prev=-1\n",
    "            for k, s in enumerate(segs):\n",
    "                segments_ids = segments_ids + [k] * (s-prev)\n",
    "                prev=s\n",
    "            segments_ids = segments_ids + [len(segs)] * (len(tokened) - len(segments_ids))\n",
    "            segments_tensors = torch.tensor([segments_ids])\n",
    "            tokens_tensor = torch.tensor([ids])\n",
    "            model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "            with torch.no_grad():\n",
    "                predictions = model(tokens_tensor, segments_tensors)\n",
    "        else:\n",
    "            print('no predictions')\n",
    "        \n",
    "        replaced = predict_word(text, predictions, maskids)\n",
    "        results[file][idx] = {'text': text, 'predictions': replaced}\n",
    "    with open(bert_results / file, 'w') as f:\n",
    "        f.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
