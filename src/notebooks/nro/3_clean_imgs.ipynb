{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import numpy as np \n",
    "from pathlib import Path \n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "from utils import create_fh_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = Path.cwd().parent.parent.parent.parent / 'processing' / 'nro_declassified' / 'imgs'\n",
    "dst = src.parent / 'cleaned_imgs'\n",
    "dst.mkdir(parents=True, exist_ok=True)\n",
    "east_weights = dst.parent.parent / 'models' / 'frozen_east_text_detection.pb'\n",
    "logs = src.parent.parent / 'logs'\n",
    "logs.mkdir(exist_ok=True)\n",
    "logger = create_fh_logger(logs / \"cleaned_imgs.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_test = r'C:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\imgs\\1959-03-02_CHARTS AUGMENTED INSTRUMENTATION IIA PROGRAMLISTS TYPES OF F_427'\n",
    "noise_test = r'C:\\Users\\brasw\\Desktop\\School\\Spring 24\\GGS 590\\project\\processing\\nro_declassified\\imgs\\1959-10-14_MEMO APPARENT DISCREPANCIES BETWEEN 117L AND C ASSEMBLY FACI_833'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_low_pass_filter(image, cutoff_frequency):\n",
    "    # Convert image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform FFT\n",
    "    f = np.fft.fft2(gray_image)\n",
    "    f_shift = np.fft.fftshift(f)\n",
    "    \n",
    "    # Apply low-pass filter\n",
    "    rows, cols = gray_image.shape\n",
    "    crow, ccol = rows // 2, cols // 2\n",
    "    mask = np.zeros((rows, cols), np.uint8)\n",
    "    mask[crow - cutoff_frequency:crow + cutoff_frequency, ccol - cutoff_frequency:ccol + cutoff_frequency] = 1\n",
    "    f_shift_filtered = f_shift * mask\n",
    "    \n",
    "    # Inverse FFT\n",
    "    f_filtered = np.fft.ifftshift(f_shift_filtered)\n",
    "    filtered_image = np.fft.ifft2(f_filtered)\n",
    "    filtered_image = np.abs(filtered_image)\n",
    "    \n",
    "    return filtered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_img = list(Path(noise_test).glob('*png'))[0]\n",
    "img = cv2.imread(str(noise_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[1500:2000, 500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( cv2.fastNlMeansDenoisingColored(img, None, 21, 21, 55, 55)[1500:2000, 500:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Identify Rotations: 0, 90, 180, 270\n",
    "The code below uses EDGE text detection to identify text for an image un-rotated, and then rotated: 90, 180, or 270 degrees. Then, non-maximum supression to filter out duplicative detections. Lastly, it does a count on detections for that rotation. \n",
    "\n",
    "The detection count per rotation will be used to identify the ideal rotation for the entire pdf. Since mirrored rotations perform similarly, a rotation will only be performed if the rotated detections are at least 10% more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/opencv/opencv/blob/7fb70e170154d064ef12d8fec61c0ae70812ce3d/samples/dnn/text_detection.py\n",
    "def decode(scores, geometry, scoreThresh):\n",
    "    detections = []\n",
    "    confidences = []\n",
    "    height = scores.shape[2]\n",
    "    width = scores.shape[3]\n",
    "    for y in range(0, height):\n",
    "        scoresData = scores[0][0][y]\n",
    "        x0_data = geometry[0][0][y]\n",
    "        x1_data = geometry[0][1][y]\n",
    "        x2_data = geometry[0][2][y]\n",
    "        x3_data = geometry[0][3][y]\n",
    "        anglesData = geometry[0][4][y]\n",
    "        for x in range(0, width):\n",
    "            score = scoresData[x]\n",
    "            if(score < scoreThresh):\n",
    "                continue\n",
    "            offsetX = x * 4.0\n",
    "            offsetY = y * 4.0\n",
    "            angle = anglesData[x]\n",
    "            cosA = math.cos(angle)\n",
    "            sinA = math.sin(angle)\n",
    "            h = x0_data[x] + x2_data[x]\n",
    "            w = x1_data[x] + x3_data[x]\n",
    "            offset = ([offsetX + cosA * x1_data[x] + sinA * x2_data[x], offsetY - sinA * x1_data[x] + cosA * x2_data[x]])\n",
    "            p1 = (-sinA * h + offset[0], -cosA * h + offset[1])\n",
    "            p3 = (-cosA * w + offset[0],  sinA * w + offset[1])\n",
    "            center = (0.5*(p1[0]+p3[0]), 0.5*(p1[1]+p3[1]))\n",
    "            detections.append((center, (w,h), -1*angle * 180.0 / math.pi))\n",
    "            confidences.append(float(score))\n",
    "    return [detections, confidences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_hw = 640\n",
    "conf_threshold = .2\n",
    "nms_threshold=.2\n",
    "available_rotations = [cv2.ROTATE_180, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, -1]\n",
    "rotations = {key: 0 for key in available_rotations} # lets use this as an accumulator to count the amount of text matches per rotation to be used as our final determinator for how to rotate the image\n",
    "net = cv2.dnn.readNetFromTensorflow(str(east_weights))\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "outNames = [\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"]\n",
    "for image in list(Path(rotation_test).glob('*png')):\n",
    "    name = image.name\n",
    "    image = cv2.imread(str(image))\n",
    "    for rotation in available_rotations:\n",
    "        frame = image.copy()\n",
    "        frame = cv2.rotate(frame, rotation) if rotation >=0 else frame\n",
    "        height_ = frame.shape[0]\n",
    "        width_ = frame.shape[1]\n",
    "        rW = width_ / float(resize_hw)\n",
    "        rH = height_ / float(resize_hw)\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (resize_hw, resize_hw), (123.68, 116.78, 103.94), True, False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(outNames)\n",
    "\n",
    "        # Get scores and geometry\n",
    "        scores = outs[0]\n",
    "        geometry = outs[1]\n",
    "        [boxes, confidences] = decode(scores, geometry, conf_threshold)\n",
    "        indices = cv2.dnn.NMSBoxesRotated(boxes, confidences, conf_threshold, nms_threshold)\n",
    "        rotations[rotation] += len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
